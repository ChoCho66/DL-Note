This is more than just a theoretical nuisance.
Consider the aforementioned one-hidden-layer MLP with two hidden units.
For illustration,
suppose that the output layer transforms the two hidden units into only one output unit.
Imagine what would happen if we initialized all of the parameters of the hidden layer as W1=c for some constant c.
In this case,
during forward propagation either hidden unit takes the same inputs and parameters,
producing the same activation,
which is fed to the output unit.
During backpropagation,
differentiating the output unit with respect to parameters W1 gives a gradient whose elements all take the same value.
Thus,
after gradient-based iteration (e.g.,
minibatch stochastic gradient descent),
all the elements of W1 still take the same value.
Such iterations would never break the symmetry on its own and we might never be able to realize the networkâ€™s expressive power.
The hidden layer would behave as if it had only a single unit.
Note that while minibatch stochastic gradient descent would not break this symmetry,
dropout regularization (to be introduced later) would!