{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"2023-01-03\"\n",
    "# subtitle: Last \n",
    "# author: <a href=\"https://github.com/ChoCho66\">ChoCho</a>\n",
    "author: ChoCho\n",
    "date: last-modified\n",
    "date-format: iso\n",
    "institute: Last update date \n",
    "bibliography: ../references.bib\n",
    "slide-number: c/t\n",
    "# knitr: true\n",
    "# jupyter: python3\n",
    "\n",
    "format:\n",
    "  revealjs:\n",
    "    # theme: beige\n",
    "    # theme: ../custom.scss\n",
    "    theme: [serif,custom.scss]    # 像 LaTeX\n",
    "    width: 1800\n",
    "    height: 1050\n",
    "    # transition: fade\n",
    "    # preview-links: auto\n",
    "    # slide-number: true\n",
    "    # slide-tone: true\n",
    "    # show-slide-number: print\n",
    "    chalkboard:\n",
    "      theme: whiteboard\n",
    "      boardmarker-width: 2\n",
    "      src: \"Chalkboard.json\"  \n",
    "    scrollable: true\n",
    "    echo: true\n",
    "    # footer: \"NCU math\"\n",
    "    # logo: cover.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d2l en ch6 Builders’ Guide\n",
    "\n",
    "- ![](https://d2l.ai/_images/blocks.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sequential Module (6.1.2 en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chocho/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<bound method Module.state_dict of MySequential(\n",
       "   (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=256, out_features=10, bias=True)\n",
       " )>,\n",
       " tensor([ 0.9646, -0.2961, -0.0681, -0.0784, -0.7608,  0.7837, -0.5562,  0.3798,\n",
       "         -0.2103, -0.6713], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output-location: column\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(\n",
    "        nn.LazyLinear(256), \n",
    "        nn.ReLU(), \n",
    "        nn.LazyLinear(10)\n",
    "    )\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "net.state_dict, net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In `pytorch`, use `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Module.state_dict of Sequential(\n",
       "   (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=256, out_features=10, bias=True)\n",
       " )>,\n",
       " tensor([ 0.0067, -0.1085, -0.6059, -0.8441, -0.1724,  0.4852, -0.4140,  1.0386,\n",
       "         -0.5748, -0.0646], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output-location: column\n",
    "net2 = nn.Sequential(\n",
    "  nn.LazyLinear(256), \n",
    "  nn.ReLU(), \n",
    "  nn.LazyLinear(10)\n",
    "  )\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "net2.state_dict, net2(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random weights which are not model parameters and thus are never updated by backpropagation (en 6.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "LazyLinear(in_features=0, out_features=4, bias=True)\n",
      "tensor([ 0.6522, -0.0703, -0.6604,  0.5343], grad_fn=<AddBackward0>)\n",
      "Linear(in_features=3, out_features=4, bias=True)\n",
      "tensor([1.4962, 1.0377, 1.0205, 1.6205, 1.5255], grad_fn=<ReluBackward0>)\n",
      "Linear(in_features=3, out_features=4, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chocho/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 3x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m net \u001b[39m=\u001b[39m FixedHiddenMLP()\n\u001b[1;32m     26\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 27\u001b[0m net\u001b[39m.\u001b[39mstate_dict, net(X)\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 20\u001b[0m, in \u001b[0;36mFixedHiddenMLP.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Reuse the fully connected layer. This is equivalent to sharing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# parameters with two fully connected layers\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(X)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(X)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear)\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 3x4)"
     ]
    }
   ],
   "source": [
    "#| output-location: column\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((4, 5))\n",
    "        self.linear = nn.LazyLinear(4)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "X = torch.arange(3).to(torch.float32)\n",
    "net.state_dict, net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "LazyLinear(in_features=0, out_features=4, bias=True)\n",
      "\n",
      "tensor([1.5987, 0.2714, 0.8514, 0.4229], grad_fn=<AddBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n",
      "tensor([3.4346, 3.5500, 2.4825, 2.7338, 2.0602], grad_fn=<ReluBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n",
      "tensor([ 1.2755, -1.1270,  0.5474,  2.1472], grad_fn=<AddBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chocho/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<bound method Module.state_dict of FixedHiddenMLP(\n",
       "   (linear): Linear(in_features=5, out_features=4, bias=True)\n",
       " )>,\n",
       " tensor(2.8432, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output-location: column\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((4, 5))\n",
    "        self.linear = nn.LazyLinear(4)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        print()\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        print()\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        print()\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        print(self.linear)\n",
    "        print()\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "X = torch.arange(5).to(torch.float32)\n",
    "net.state_dict, net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2760, -2.0931, -3.4741, -5.0602, -0.9771], grad_fn=<AddBackward0>)\n",
      "LazyLinear(in_features=0, out_features=4, bias=True)\n",
      "\n",
      "tensor([ 0.7497,  1.5311, -0.5474, -1.3877], grad_fn=<AddBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n",
      "tensor([0.6991, 1.7620, 1.2413, 0.0840, 1.9946], grad_fn=<ReluBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n",
      "tensor([ 0.4324,  0.0697, -0.1702,  0.1354], grad_fn=<AddBackward0>)\n",
      "Linear(in_features=5, out_features=4, bias=True)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('0.net.0.weight',\n",
       "               Parameter containing:\n",
       "               tensor([[-0.0036,  0.0389, -0.0717, -0.0846, -0.0710, -0.0936,  0.0365, -0.1154,\n",
       "                        -0.0454,  0.0805,  0.0005, -0.0640, -0.1398, -0.0549, -0.0640, -0.0317,\n",
       "                         0.1289,  0.0568,  0.1242,  0.0101, -0.0691,  0.1232, -0.0208,  0.0289,\n",
       "                        -0.1040,  0.0653,  0.0312,  0.0572, -0.0080,  0.1268, -0.0524,  0.0817,\n",
       "                         0.0638,  0.1405,  0.1176, -0.0147, -0.1191,  0.0176, -0.0174, -0.0558,\n",
       "                        -0.1079,  0.0407,  0.0873,  0.0093, -0.0576, -0.0788, -0.0341,  0.0834,\n",
       "                        -0.0486,  0.1231],\n",
       "                       [ 0.0355, -0.0237, -0.0801, -0.0782, -0.0246,  0.1300, -0.1150, -0.0709,\n",
       "                        -0.0187, -0.0645, -0.0414, -0.0435, -0.0042,  0.0096, -0.0184,  0.1130,\n",
       "                         0.0514, -0.0007,  0.0482, -0.1079,  0.0327, -0.0856, -0.0564, -0.0356,\n",
       "                        -0.0899, -0.1351,  0.1039, -0.1277,  0.0967,  0.0340,  0.0686,  0.0809,\n",
       "                         0.0318,  0.1279, -0.0651,  0.0052,  0.0350,  0.0146,  0.0251,  0.0688,\n",
       "                        -0.0720,  0.0545,  0.0912, -0.0411,  0.0297, -0.0251,  0.1348, -0.0542,\n",
       "                        -0.0712,  0.0298],\n",
       "                       [-0.0095, -0.0352,  0.0497,  0.1284, -0.1227,  0.1390, -0.1151, -0.1313,\n",
       "                         0.0906, -0.0276,  0.0349, -0.1353, -0.1062, -0.1003, -0.0301,  0.0104,\n",
       "                        -0.1007,  0.1072, -0.0495,  0.0232, -0.0974,  0.0075,  0.0587,  0.1164,\n",
       "                        -0.1112,  0.0693,  0.0459, -0.0058, -0.1176, -0.1072, -0.0522,  0.0349,\n",
       "                        -0.1083, -0.0482, -0.0024,  0.1294, -0.0589, -0.1274, -0.0562, -0.0121,\n",
       "                         0.0766, -0.0102, -0.0474,  0.0400,  0.0891,  0.1253, -0.0654, -0.1280,\n",
       "                        -0.0919, -0.0275]], requires_grad=True)),\n",
       "              ('0.net.0.bias',\n",
       "               Parameter containing:\n",
       "               tensor([-0.0321, -0.0101, -0.1392], requires_grad=True)),\n",
       "              ('0.net.2.weight',\n",
       "               Parameter containing:\n",
       "               tensor([[-0.0873, -0.4175, -0.1453],\n",
       "                       [ 0.3615,  0.3476, -0.3141],\n",
       "                       [-0.3749, -0.1012, -0.1466],\n",
       "                       [ 0.2164,  0.1193, -0.4482]], requires_grad=True)),\n",
       "              ('0.net.2.bias',\n",
       "               Parameter containing:\n",
       "               tensor([ 0.5597, -0.4830, -0.2581,  0.1108], requires_grad=True)),\n",
       "              ('0.linear.weight',\n",
       "               Parameter containing:\n",
       "               tensor([[ 0.1423, -0.4839, -0.3407, -0.3875],\n",
       "                       [ 0.1804,  0.3797, -0.1317,  0.3983]], requires_grad=True)),\n",
       "              ('0.linear.bias',\n",
       "               Parameter containing:\n",
       "               tensor([0.4685, 0.2446], requires_grad=True)),\n",
       "              ('1.weight',\n",
       "               Parameter containing:\n",
       "               tensor([[-0.3333, -0.2567],\n",
       "                       [ 0.5444,  0.1476],\n",
       "                       [ 0.3966, -0.4194],\n",
       "                       [ 0.3223, -0.6990],\n",
       "                       [-0.2852, -0.6532]], requires_grad=True)),\n",
       "              ('1.bias',\n",
       "               Parameter containing:\n",
       "               tensor([-0.0575, -0.3424,  0.1402, -0.5331,  0.6620], requires_grad=True)),\n",
       "              ('2.linear.weight',\n",
       "               Parameter containing:\n",
       "               tensor([[ 0.4148,  0.0780, -0.3944,  0.1735,  0.0585],\n",
       "                       [-0.1296, -0.0195, -0.0859, -0.2429,  0.1086],\n",
       "                       [ 0.3038, -0.3254,  0.3180,  0.1807, -0.3115],\n",
       "                       [-0.0239,  0.1734,  0.2315,  0.2067, -0.4342]], requires_grad=True)),\n",
       "              ('2.linear.bias',\n",
       "               Parameter containing:\n",
       "               tensor([0.3632, 0.1050, 0.4023, 0.4078], requires_grad=True))]),\n",
       " tensor(0.4673, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output-location: column\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "          nn.LazyLinear(3), nn.ReLU(),\n",
    "          nn.LazyLinear(4), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(\n",
    "  NestMLP(), \n",
    "  nn.LazyLinear(5), \n",
    "  FixedHiddenMLP()\n",
    "  )\n",
    "\n",
    "X = torch.arange(50).to(torch.float32)\n",
    "chimera.state_dict(), chimera(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Net` 共用參數 (6.2.2 en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3106,  0.1066,  0.2790, -0.2405, -0.1878])\n",
      "tensor([ 0.3106,  0.1066,  0.2790, -0.2405, -0.1878])\n",
      "\n",
      "tensor([ 0.1000,  0.1066,  0.2790, -0.2405, -0.1878])\n",
      "tensor([ 0.1000,  0.1066,  0.2790, -0.2405, -0.1878])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chocho/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "\n",
    "m = 5  # 下面兩個 m 值一定要一樣\n",
    "shared = nn.LazyLinear(m)\n",
    "net = nn.Sequential(nn.LazyLinear(m), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0])\n",
    "print(net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 0.1\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print()\n",
    "print(net[2].weight.data[0])\n",
    "print(net[4].weight.data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization (6.3 en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.2723, -0.2298, -0.3999],\n",
       "         [-0.4484, -0.5352, -0.2473],\n",
       "         [ 0.1999,  0.5098, -0.2889],\n",
       "         [ 0.3707, -0.3990, -0.2375]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2529, 0.2601, 0.3132, 0.0876], requires_grad=True),\n",
       " tensor([-1.1342, -2.0006,  0.6661, -1.0522], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear(3,4)\n",
    ")\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "net[0].weight, net[0].bias, net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 5.7912e-01,  7.2905e-01,  2.7349e-01],\n",
       "         [ 8.2351e-01,  9.2833e-01, -6.2403e-04],\n",
       "         [-4.6876e-02, -1.6262e-01, -1.1401e+00],\n",
       "         [-1.1399e+00,  3.3388e-01, -8.0548e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0.], requires_grad=True),\n",
       " tensor([ 2.8577,  2.6783, -3.7925, -2.8886], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear(3,4)\n",
    ")\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "nn.init.normal_( net[0].weight , 0, 1 )\n",
    "nn.init.zeros_( net[0].bias )\n",
    "net[0].weight, net[0].bias, net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[6., 6., 6.],\n",
       "         [6., 6., 6.],\n",
       "         [6., 6., 6.],\n",
       "         [6., 6., 6.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2872, -0.4617, -0.3226,  0.0261], requires_grad=True),\n",
       " tensor([36.2872, 35.5383, 35.6774, 36.0261], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear(3,4)\n",
    ")\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "nn.init.constant_( net[0].weight , 6 )\n",
    "net[0].weight, net[0].bias, net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.3272, -0.6721, -0.7902],\n",
       "         [ 0.8782,  0.3942, -0.8563],\n",
       "         [-0.0585,  0.3681, -0.2382],\n",
       "         [ 0.4020,  0.6003, -0.2567]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4035,  0.0404, -0.2751,  0.5587], requires_grad=True),\n",
       " tensor([-3.6384, -0.8621, -0.3121,  1.3913], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear(3,4)\n",
    ")\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "nn.init.xavier_uniform_( net[0].weight )\n",
    "net[0].weight, net[0].bias, net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Initialization (6.3.1.1 en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([4, 3])\n",
      "Init weight torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 1.2087, -7.8388, -4.4931],\n",
       "         [ 0.3281,  2.8136, -9.8746],\n",
       "         [-3.1472, -6.7930,  3.1936],\n",
       "         [ 6.7071, -9.9570, -3.0954]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2979, -0.4321,  0.3465, -0.2627], requires_grad=True),\n",
       " tensor([ 0.1822, -0.1544], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Linear(3,4), nn.ReLU(),\n",
    "  nn.Linear(4,5), nn.ReLU(),\n",
    "  nn.LazyLinear(2), \n",
    ")\n",
    "X = torch.FloatTensor([1,2,3])\n",
    "\n",
    "def my_init(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in module.named_parameters()][0])\n",
    "        nn.init.uniform_(module.weight, -10, 10)\n",
    "        # module.weight.data *= module.weight.data.abs() >= 5\n",
    "net.apply(my_init)\n",
    "net[0].weight, net[0].bias, net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀察 Net 的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7245706daf4d43e5add5ef74695c420c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='w11', max=10.0, min=-10.0), FloatSlider(value=-1.0, …"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 16\n",
    "W1 = nn.Linear(1,m)\n",
    "W1.weight.requires_grad = False\n",
    "\n",
    "W2 = nn.Linear(m,m)\n",
    "W2.weight.requires_grad = False\n",
    "\n",
    "Wn = nn.Linear(m,m)\n",
    "Wn.weight.requires_grad = False\n",
    "\n",
    "sigR = nn.ReLU()\n",
    "sigS = nn.Sigmoid()\n",
    "\n",
    "Net = nn.Sequential(\n",
    "  W1, sigR,\n",
    "  nn.Linear(m,m), sigR,\n",
    "  W2, sigR,\n",
    "  nn.Linear(m,m), sigR,\n",
    "  Wn, sigR,\n",
    "  nn.Linear(m,1),\n",
    ")\n",
    "\n",
    "def f(w11=2.0, w21=-1.0, wn1=2.0, b=0.0):  \n",
    "  W1.weight[0,0] = torch.tensor([w11])\n",
    "  W2.weight[0,0] = torch.tensor([w21])\n",
    "  Wn.weight[0,0] = torch.tensor([wn1])\n",
    "  \n",
    "  plt.figure(2)\n",
    "  x = np.linspace(-16, 16, num=600)\n",
    "  y = torch.zeros_like(torch.zeros(len(x)))\n",
    "  x_torch = torch.from_numpy(x).to(torch.float32)\n",
    "  # np.linspace(-10, 10, num=100000)\n",
    "  for j in range(len(x_torch)):\n",
    "    y[j] = Net(x_torch[j:j+1])\n",
    "  y = y.detach().numpy() + b\n",
    "  # print(x[:16])\n",
    "  print(y[:16])\n",
    "  plt.plot(x, y)\n",
    "  plt.ylim(-1, 1)\n",
    "  plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, w11 = (-10.0, 10.0),\n",
    "                                  w21 = (-10.0, 10.0),\n",
    "                                  wn1 = (-10.0, 10.0),\n",
    "                                  b = (-20.0, 20.0)\n",
    "                                  )\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '500px'\n",
    "interactive_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for MLP\n",
    "\n",
    "- Deep vs Shallow\n",
    "  - https://youtu.be/FN8jclCrqY0?t=1674\n",
    "\n",
    "- 第一層較為重要\n",
    "  - https://youtu.be/FN8jclCrqY0?t=2032\n",
    "  - https://vigneshgig.medium.com/why-first-hidden-layer-is-very-important-in-building-a-neural-network-model-and-relation-between-6f2943acc847\n",
    "\n",
    "- 如何確定神經網絡的層數和隱藏層神經元數量.\n",
    "  - https://zhuanlan.zhihu.com/p/100419971\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN RNN\n",
    "\n",
    "- Find the features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. 学习表征 zh\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://vigneshgig.medium.com/why-first-hidden-layer-is-very-important-in-building-a-neural-network-model-and-relation-between-6f2943acc847\n",
    "\n",
    "- https://www.youtube.com/watch?v=FN8jclCrqY0\n",
    "- https://www.youtube.com/watch?v=qpuLxXrHQB4\n",
    "\n",
    "大家好，\n",
    "我將解釋“為什麼第一個隱藏層在構建神經網絡模型中非常重要”，\n",
    "我還將解釋激活函數如何解決梯度消失問題。\n",
    "我將使用 google playground.tensorflow 來解釋這個概念，\n",
    "這是一個非常棒的工具，\n",
    "可以可視化神經網絡模型的內部工作部分。\n",
    "我建議您嘗試一下，\n",
    "這樣您將獲得神經網絡背後的更多直覺。\n",
    "\n",
    "在我之前的博客中，\n",
    "我介紹了為什麼在神經網絡中使用激活函數以及第一個隱藏層，\n",
    "所以我不打算在這個博客中解釋。\n",
    "我建議您閱讀我的為什麼在神經網絡中使用激活函數，\n",
    "這將提供有關激活函數和第一個隱藏層的更多信息。\n",
    "\n",
    "Case 1\n",
    "\n",
    "為了向您展示為什麼第一個隱藏層更重要，\n",
    "我將限制第一個隱藏層只有兩個神經元，\n",
    "然後我們可以在第一個隱藏層之後添加許多不受神經元限制的隱藏層，\n",
    "如果您使用的是 sigmoid 函數，\n",
    "則再添加一個不超過兩個隱藏層，\n",
    "我們最終可能會以消失的梯度結束。\n",
    "如果你想檢查兩個以上的隱藏層，\n",
    "請使用 relu 激活函數，\n",
    "這將避免梯度消失問題，\n",
    "不管怎樣，\n",
    "我打算同時使用 sigmoid 和 ReLu 激活函數。\n",
    "\n",
    "正如我們所見，\n",
    "即使我們使用了三個隱藏層，\n",
    "該模型也無法完全分離或分類數據集。\n",
    "所以我們在第一個隱藏層中至少需要三個 3 神經元。\n",
    "在數學術語中，\n",
    "我們至少需要三個線性方程來分離這個非線性數據集，\n",
    "或者在拓撲術語中，我們需要多一個維度來將低維轉換為高維，\n",
    "以便我們可以在高維中線性分離非線性數據集，\n",
    "在這種情況下，\n",
    "我們有 2d 數據集，\n",
    "所以我們需要多一維，\n",
    "這樣我們就可以將 2d 非線性數據集轉換為 3d 線性數據集。\n",
    "\n",
    "Case 2\n",
    "\n",
    "瞧！\n",
    "它僅用 3 個隱藏參數就完全分離或分類了數據集。\n",
    "如果你想知道為什麼請閱讀我的博客為什麼在神經網絡中使用激活函數。\n",
    "所以如果你對神經網絡建模，\n",
    "你應該更加重視第一個隱藏層，\n",
    "因為所有其他隱藏層都依賴於第一個隱藏層。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於梯度消失，\n",
    "一層神經元的學習率逐層降低，\n",
    "因此神經元的權重不變。\n",
    "例如，\n",
    "當我們有四個隱藏層時。\n",
    "假設第 4 個隱藏層學習率為 0.9，\n",
    "那麼第 3 個隱藏層為 0.5，\n",
    "第 2 個隱藏層為 0.1，\n",
    "第 1 個隱藏層學習率為 0.001，\n",
    "由於第一個隱藏層沒有學習，\n",
    "錯誤率沒有降低。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9.3 分布偏移"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perturbation at a layer grows exponentially in the remaining depth after that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "924b4310eef0a4626e6be183858539210ebddb9168f6747ddef9ed01eb54dbfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
